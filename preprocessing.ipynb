{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /Users/Soda/opt/anaconda3/lib/python3.9/site-packages (0.0.post5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Soda/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/Soda/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/Soda/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/Soda/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /Users/Soda/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "!pip install sklearn\n",
    "import nltk\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "# Download the NLTK stop words\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "# Load the NLTK stop words\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction\n",
      "\n",
      "\n",
      "Of course, she was wearing the sunglasses.\n",
      "\n",
      "Anna Wintour walked into the Vogue staff \n"
     ]
    }
   ],
   "source": [
    "# Read the original text from the book\n",
    "\n",
    "# Open the text file in read mode\n",
    "file = open('origin.txt', 'r')\n",
    "\n",
    "# Read the entire content of the file\n",
    "content = file.read()\n",
    "\n",
    "# Close the file\n",
    "file.close()\n",
    "\n",
    "# Print the content\n",
    "print(content[:100])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ORIGINS\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Born Eleanor Baker to a wealthy Quaker family in 1917 in Harrisburg, Pennsylvania, the \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# use regular expression to find chapter headers\n",
    "pattern = r'\\n\\n(Chapter \\d+)'\n",
    "\n",
    "# split the text into chapters by headers\n",
    "sections = re.split(pattern, content)\n",
    "\n",
    "# Remove leading/trailing whitespace and group chapters with their content\n",
    "chapters = [sections[i] + sections[i + 1] for i in range(0, len(sections) - 1, 2)]\n",
    "print(chapters[1][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=list(stop_words))\n",
    "\n",
    "# calculate the TF-IDF scores for the chapters\n",
    "tfidf_scores = vectorizer.fit_transform(chapters)\n",
    "\n",
    "# get the vocabulary\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "# get the TF-IDF scores for the first chapter\n",
    "chapter_scores = tfidf_scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "son\n",
      "evening\n",
      "standard\n",
      "anna\n",
      "war\n",
      "would\n",
      "gerald\n",
      "beaverbrook\n",
      "nonie\n",
      "charles\n"
     ]
    }
   ],
   "source": [
    "# print the top 10 words with the highest TF-IDF score\n",
    "\n",
    "# convert the IDF scores for the first chapter into a numpy array\n",
    "# chapter_scores = chapter_scores.toarray()\n",
    "\n",
    "# get the indices for the words with the highest TF-IDF scores\n",
    "top_10_indices = np.argsort(chapter_scores.toarray())[0][-10:]\n",
    "\n",
    "# get the words with the highest TF-IDF scores\n",
    "top_10_words = [vocabulary[index] for index in top_10_indices]\n",
    "\n",
    "# print the top 10 words with the highest TF-IDF scores\n",
    "for word in top_10_words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Introduction', 'Of', 'course', ',', 'wearing', 'sunglasses', '.', 'Anna', 'Wintour', 'walked', 'Vogue', 'staff', 'meeting', 'looked', 'group', 'gathered', 'around', 'table', 'ten', 'thirty', 'morning', '.', 'Many', 'working', 'late', 'night', ',', 'coming', 'stories', 'attempting', 'explain', 'unprecedented', '.', 'Others', 'crying', ',', 'scared', ',', 'shock', '.', 'Anna', 'extraordinary', 'influence', 'great', 'many', 'things', ',', 'outcome', 'election', 'one', '.', 'It', 'November', '9', ',', '2016', '.', 'Despite', 'Hillary', 'Clinton', '’', 'loss', ',', 'Vogue', '’', 'full-throttle', 'support', ',', 'including', 'endorsement—the', 'first', 'kind', 'magazine', '’', '124-year', 'history—Anna', 'started', 'day', 'usual', '.', 'She', 'rose', '5:00', 'a.m.', ',', 'exercised', '5:30', '6:00', '(', 'depending', 'whether', 'played', 'twice', 'weekly', 'tennis', 'worked', 'trainer', ')', ',', 'sat']\n",
      "('Introduction', 'NN')\n",
      "('Of', 'IN')\n",
      "('course', 'NN')\n",
      "(',', ',')\n",
      "('she', 'PRP')\n",
      "('was', 'VBD')\n",
      "('wearing', 'VBG')\n",
      "('the', 'DT')\n",
      "('sunglasses', 'NNS')\n",
      "('.', '.')\n",
      "('Anna', 'NNP')\n",
      "('Wintour', 'NNP')\n",
      "('walked', 'VBD')\n",
      "('into', 'IN')\n",
      "('the', 'DT')\n",
      "('Vogue', 'NNP')\n",
      "('staff', 'NN')\n",
      "('meeting', 'NN')\n",
      "('and', 'CC')\n",
      "('looked', 'VBD')\n",
      "('at', 'IN')\n",
      "('the', 'DT')\n",
      "('group', 'NN')\n",
      "('that', 'WDT')\n",
      "('had', 'VBD')\n",
      "('gathered', 'VBN')\n",
      "('around', 'IN')\n",
      "('the', 'DT')\n",
      "('table', 'NN')\n",
      "('about', 'IN')\n",
      "('ten', 'JJ')\n",
      "('thirty', 'NN')\n",
      "('that', 'IN')\n",
      "('morning', 'NN')\n",
      "('.', '.')\n",
      "('Many', 'NNP')\n",
      "('of', 'IN')\n",
      "('them', 'PRP')\n",
      "('had', 'VBD')\n",
      "('been', 'VBN')\n",
      "('working', 'VBG')\n",
      "('late', 'RB')\n",
      "('into', 'IN')\n",
      "('the', 'DT')\n",
      "('night', 'NN')\n",
      "(',', ',')\n",
      "('coming', 'VBG')\n",
      "('up', 'RP')\n",
      "('with', 'IN')\n",
      "('stories', 'NNS')\n",
      "('attempting', 'VBG')\n",
      "('to', 'TO')\n",
      "('explain', 'VB')\n",
      "('the', 'DT')\n",
      "('unprecedented', 'JJ')\n",
      "('.', '.')\n",
      "('Others', 'NNS')\n",
      "('had', 'VBD')\n",
      "('just', 'RB')\n",
      "('been', 'VBN')\n",
      "('up', 'RP')\n",
      "('crying', 'VBG')\n",
      "(',', ',')\n",
      "('scared', 'VBN')\n",
      "(',', ',')\n",
      "('in', 'IN')\n",
      "('shock', 'NN')\n",
      "('.', '.')\n",
      "('Anna', 'NNP')\n",
      "('had', 'VBD')\n",
      "('extraordinary', 'JJ')\n",
      "('influence', 'NN')\n",
      "('over', 'IN')\n",
      "('a', 'DT')\n",
      "('great', 'JJ')\n",
      "('many', 'JJ')\n",
      "('things', 'NNS')\n",
      "(',', ',')\n",
      "('but', 'CC')\n",
      "('the', 'DT')\n",
      "('outcome', 'NN')\n",
      "('of', 'IN')\n",
      "('this', 'DT')\n",
      "('election', 'NN')\n",
      "('was', 'VBD')\n",
      "('not', 'RB')\n",
      "('one', 'CD')\n",
      "('of', 'IN')\n",
      "('them', 'PRP')\n",
      "('.', '.')\n",
      "('It', 'PRP')\n",
      "('was', 'VBD')\n",
      "('November', 'NNP')\n",
      "('9', 'CD')\n",
      "(',', ',')\n",
      "('2016', 'CD')\n",
      "('.', '.')\n",
      "('Despite', 'IN')\n",
      "('Hillary', 'JJ')\n",
      "('Clinton', 'NNP')\n",
      "['Introduction', 'Of', 'course', ',', 'she', 'be', 'wear', 'the', 'sunglass', '.', 'Anna', 'Wintour', 'walk', 'into', 'the', 'Vogue', 'staff', 'meeting', 'and', 'look', 'at', 'the', 'group', 'that', 'have', 'gather', 'around', 'the', 'table', 'about']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "tokens = word_tokenize(content)\n",
    "\n",
    "# Remove stop words\n",
    "filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# Print the filtered tokens\n",
    "print(filtered_tokens[:100])\n",
    "\n",
    "# get POS tags for the tokens\n",
    "from nltk import pos_tag\n",
    "words_with_pos = nltk.pos_tag(tokens)\n",
    "i = 0  \n",
    "for w in words_with_pos:\n",
    "    print(w)\n",
    "    i += 1\n",
    "    if i == 100:\n",
    "      break\n",
    "\n",
    "# use a function to format the pos tag that needs to lemmatize\n",
    "def get_wordnet_pos(tag):\n",
    "  if tag.startswith('J'):\n",
    "    return wn.ADJ\n",
    "  elif tag.startswith('V'):\n",
    "    return wn.VERB\n",
    "  elif tag.startswith('N'):\n",
    "    return wn.NOUN\n",
    "  elif tag.startswith('R'):\n",
    "    return wn.ADV\n",
    "  else:\n",
    "    return None\n",
    "  \n",
    "# for each word in the tokens, lemmatize them by the tags, and put them in a new list\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = []\n",
    "for word, tag in words_with_pos:\n",
    "  wordnet_pos = get_wordnet_pos(tag)\n",
    "  if wordnet_pos is not None:\n",
    "    lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "  else:\n",
    "    lemma = word\n",
    "  lemmatized_words.append(lemma)\n",
    "\n",
    "# show the first 30 words of lemmatized words\n",
    "print(lemmatized_words[:30:1])  \n",
    "# Save the tokens to a file for later use\n",
    "# with open('tokens.pkl', 'wb') as f:\n",
    "#     pickle.dump(filtered_tokens, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
